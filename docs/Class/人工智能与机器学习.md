---
typora-copy-images-to: ..\picture
---

# 人工智能与机器学习

???+note "课程资源"
    === "作业资源"
       看到是国科大的作业<br>
       [作业](https://wenku.baidu.com/view/a008b8323a3567ec102de2bd960590c69ec3d8e1.html?_wkts_=1727330708826&needWelcomeRecommand=1)<br>
       [第二章作业](https://blog.csdn.net/m0_67495624/article/details/135541806)<br>
       [小测1](https://www.cc98.org/topic/5795381)
       [小测2](https://www.cc98.org/topic/6006987)

    === "分数构成"
        15% 随堂测试<br>
        20% 实验报告<br>
        5% 考勤<br>
        8% 课后作业<br>
        12% 小组大作业<br>
        40% 闭卷考试






## 绪论

### 定义

**人工智能**：Artificial Intelligence，目前是指用**计算机模拟或实现的智能**， 研究如何在机器上实现**人类智能**。即用机器来模仿人的智能。因此人工智能**又称机器智能**

**人类的自然智能**：指人在认识客观世界的过程中，由**思维过程和脑力活动**所表现出来的综合能力，如感知观察能力、记忆能力、逻辑思维能力和语言表达能力等

智能包含的能力

- 感知能力
- 记忆和思维能力
- 学习和自适应能力
- 行为能力	<br>
$\quad$1.含义：是人们对感知到的外界信息作出动作反应的能力<br>
$\quad$2.信息来源：由感知直接获得的外界信息经过思维加工后的信息。

人工智能的简单模型

![image-20240926145246010](../picture/image-20240926145246010.png)

详细定义：人工智能是研究人类智能活动的规律，构造具有一定智能的**人工系统**，研究如何让**计算机**去完成以往需要人的智力才能胜任的工作，也就是研究如何应用计算机的软硬件来模拟人类某些智能行为的基本理论、方法和技术。



## 智能AGENT

### AGENT的本质

#### 基本定义

智能体（Agent）：能够感知和动作的实体（任何独立的能够“思想”并可以同环境交互的实体都可以抽象为智能体）

![image-20240926145810456](../picture/image-20240926145810456.png)

简单说，一个智能体就是从感知序列到动作的一个函数： $f:P^* -> A$

- 感知信息：表示任何时刻Agent的感知输入。感

- 知序列：Agent收到的所有输入数据的完整历史。<br>
  $\quad$Agent在任何时刻的行动选择，取决于到该时刻为止的整个感知序列。<br>

- Agent函数：将任意给定感知序列映射到Agent的动作。可以描述Agent的行为。

#### Agent函数与Agent程序

- Agent函数：是一个抽象的数学表示
- Agent程序： 是Agent函数在某种物理体系上的具体实现

**Example**

![image-20240926152028566](../picture/image-20240926152028566.png)

- 世界：方格A和B
- 感知：可以感知所处的方格（位置）、是否有灰尘（本地的状态）如：[A, Dirty]
- 动作：向左移动、向右移动、吸取灰尘、什么也不做	Left, Right, Suck, NoOp
- Agent函数（感知-动作映射函数）：如果当前地点有灰尘，则吸取。否则移动到另一地点。

| **感知序列** | **行动** |
| :----------: | :------: |
|  [A, Clean]  |  Right   |
|  [A, Dirty]  |   Suck   |
|  [B, Clean]  |   Left   |
|  [B, Dirty]  |   Suck   |

### 评价Agent的行为

- 理性Agent是做事正确的智能体。<br>
$\quad$1.即Agent函数表格的右边都填写正确。<br>
$\quad$2.严谨定义：对于每一个可能的**感知序列**，根据已知的**感知序列**和内建的**先验知识**，理性Agent选择能使**性能指标**的期望值**最大化的动作**。<br>

- 性能度量（Agent成功程度的标准）：通常由理性Agent的设计者给出，根据实际在所处的环境中希望得到的结果来设计度量，而不是根据智能体表现的行为。	<br>
$\quad$ 1.把Agent置于一个环境中后，它将针对收到的感知信息产生动作序列。	<br>
$\quad$ 2.该动作序列引起环境历经一个状态序列。	<br>
$\quad$ 3.如果环境的上述状态序列是想要的，则Agent的性能良好。<br>

- 理性的衡量依据：PEAS<br>
$\quad$ 1.性能度量——performance<br>
$\quad$ 2.agent对环境的先验知识——environment<br>
$\quad$ 3. agent可以执⾏的动作——actions<br>
$\quad$ 4.agent的感知序列——sensor<br>

- 理性不完美，理性不全知
- 理性智能体要会“学习”
- 理性智能体要有“自主性”

### 任务环境

#### 性质

- 完全可观察与部分可观察<br>
$\quad$ 1.完全可观察：智能体能获取环境的完整状态，智能体不需要内部维护来记录世界的状况
- 确定性与随机<br>
$\quad$ 2.确定性：环境的下一个状态完全取决于当前状态和智能体的行动<br>
$\quad$ 3. 环境是确定的，除非有其它智能体活动的影响。<br>
- 片段式与延续式：行动的选择是否取决于当前片段
- 静态与动态<br>
$\quad$ 1.环境在智能体思考的时候是否会变化<br>
$\quad$ 2.出租车驾驶是动态的；纵横字谜游戏是静态的<br>
$\quad$ 3.半动态：环境本身不随时间的流逝而变化，但智能体的性能评价随时间变化；例如计时棋赛。<br>
- 离散与连续：智能体感知的信息和行动
- 单智能体与多智能体

**真实的世界**是部分可观，随机的，延续式的，动态的，连续的，多智能体的。



### Agent的结构

智能体 = 体系结构 + 程序

- AI的任务是设计智能体**程序**，实现把感知信息映射到行动的智能体**函数f**
- 体系结构为程序提供：<br>
$\quad$ 1.来自传感器的感知信息<br>
$\quad$ 2.运行程序<br>
$\quad$ 3.把程序产生的行动送到执行器 <br>
- 所选择的程序必须适合体系结构

???+note "判断题"
    === "每个Agent函数都可以由机器/程序组合呈现"
	False。受机器的运算能力和存储能力限制。<br>
	=== "一个Agent函数可能对应多个Agent程序"
	True。Agent程序与运行平台关联<br>
	=== "实现给定Agent函数的Agent程序是否可以有多个"
	True。比如添加一些代理程序，只输入不会影响输出的空值。<br>
	=== "给定一个固定的机器结构，任意agent程序都会精确执行一个agent函数吗？"
	True。agent的行为被结构和程序固定。<br>
	
!!! note "设想我们现在保持agent程序固定，但是我们加快机器运行速度为两倍，会影响agent函数吗？"
	答:这取决于程序和环境。 如果环境是动态的，则加快机器速度可能意味着选择不同（也许更好）的动作。 如果环境是静态的，并且程序不在意运行时间，agent函数保持不变。<br>
	
	

### 智能体的（结构）类型

#### 分类

简单反射型：基于当前感知、忽略历史感知<br>

基于模型的简单反射型：使用内部模型记录世界的当前状态，按反射型决策<br>

基于目标型：追踪记录世界状态、要达到的目标，选择导致达成目标的行为<br>

基于效用型：使用更普适的度量：效用函数，把状态映射到实数来描述与智能体与状态相关的高兴程<br>



- 每种Agent程序都以特定的方式结合了特定的成分来生成行动
- 所有这些智能体都可以转变为学习智能体，能够提高性能以便生成更好的行动



##### 简单反射智能体

- 基于当前感知选择行动，忽略感知历史<br>
$\quad$ 1.基于当前感知选择行动，忽略感知历史<br>
$\quad$ 2.只有在可以仅根据当前感知信息来完成当前决策的情况下才能工作，即环境完全可观<br>

![image-20240926184333777](../picture/image-20240926184333777.png)

##### 具有模型的反射智能体

- 使用内部模型记录世界的当前状态，按反射型智能体方式选择行动

![image-20240926184340153](../picture/image-20240926184340153.png)

##### 基于目标的智能体

追踪记录世界的状态、要达到的目标，并选择导致达成目标的行动

![image-20240926184344721](../picture/image-20240926184344721.png)

##### 基于效用（utility）的智能体

使⽤更普适的度量：**效⽤函数**，把状态映射到实数来描述与智能体与状态相关的⾼兴程度

效用函数可以辅助进行决策：

- 有多个相互冲突的目标可达到时实现折中
- 多个目标都不能有把握达到时选择一个目标

![image-20240926184354139](../picture/image-20240926184354139.png)

##### 学习智能体

- 学习元件：负责改进，利用评论元件的反馈来评价智能体，并决定如何修改执行元件以将来做得更好
- 执行元件：负责选择外部动作，接受感知信息并进行行动决策
- 评论元件：根据固定的性能标准告诉学习元件智能体的运转情况如何
- 问题产生器：负责提议可以导致新的和有信息价值的经验的行动

![image-20240926184748107](../picture/image-20240926184748107.png)

Agent程序包含回答以下问题的部件：

- 1）当前状态；
- 2）当前应该采取的行动；
- 3）行动后果



智能体特性

- 自主性
- 反应性
- 适应性
- 社会性



### 总结

- 定义：智能体是可以感知环境并在环境中行动的某种东西。智能体函数指定智能体响应任何感知序列所采取的行动。
- 性能度量评价智能体在环境中的行为表现。理性智能体的行动使其性能度量期望值最大化。
- 任务环境包括性能度量、外部环境、执行器和传感器。设计智能体的第一步总是把任务空间定义得尽可能完全。
- 任务环境的性质：变化的，完全可观察的？确定性的？片段式的？静态的？离散的？单智能体的？
- 智能体程序是智能体函数的实现
- 简单反射型智能体直接对感知信息作出反应
- 基于模型的反射智能体保持内部状态，追踪记录当前感知信息中不明显的世界信息。
- 基于目标的智能体的行动为了达到目标
- 基于效用的智能体试图最大化自己期望的“快乐”
- 所有智能体都可以通过学习来改进其性能


## 贪婪算法

### 广度优先搜索算法



### 深度优先搜索算法



### A*算法

[A*讲解](https://blog.csdn.net/Zhouzi_heng/article/details/115035298)

####  搜索区域(The Search Area)

以题目进行解释，我们假设某人要从 A 点移动到 B 点，但是这两点之间被一堵墙隔开。如图 1 ，绿色是 A ，红色是 B ，中间蓝色是墙。

![img](../picture/0b78c760ac45ec2f8e30745201fecad7.jpeg)

格子的状态分为可走 (walkalbe) 和不可走 (unwalkable)

#### 开始搜索(Starting the Search)

- 从起点 A 开始，并把它就加入到一个由方格组成的 open list( 开放列表 ) 中。 Open list 里的格子是路径可能会是沿途经过的，也有可能不经过。基本上 open list 是一个**待检查**的方格列表。
- 查看与起点 A 相邻的方格 ( 忽略其中unwalk的方格 ) ，把其中可走的 (walkable) 或可到达的 (reachable) 方格也加入到 open list 中。把起点 A 设置为这些方格的父亲 (parent node 或 parent square) 。
- 把 A 从 open list 中移除，加入到 close list( 封闭列表 ) 中， close list 中的每个方格都是现在不需要再关注的。

如下图所示，深绿色的方格为起点，它的外框是亮蓝色，表示该方格被加入到了 close list 。与它相邻的黑色方格是需要被检查的，他们的外框是亮绿色。每个黑方格都有一个灰色的指针指向他们的父节点，这里是起点 A 。

![image002.jpg](../picture/c232b6a651f1a54116fa38e7b6de142a.jpeg)

#### 路径排序(Path Sorting)

对每个节点，在计算时同时考虑两项**代价**指标：**当前节点与起始点的距离**，以及**当前节点与目标点的距离**：F = G + H

- **欧式距离**：G = 从起点 A 移动到指定方格的移动代价，沿着到达该方格而生成的路径。
  - $G = \sqrt{(x_1 - x_2)^2 +(y_1 - y_2)^2}$ 
- **曼哈顿距离**：H = 从指定的方格移动到终点 B 的估算成本。
  - $H = |x_1 - x_2| + |y_1 - y_2|$
  - 注意，H函数的选取要满足**估算成本小于实际成本**

计算起始点相邻方格的F、G、H的值，分别记录在左上角，左下角和右下角

![image003.jpg](../picture/908e62d12ad781ced1f767c945c1feda.jpeg)

#### 继续搜索(Continuing the Search)

为了继续搜索，我们从 open list 中选择 F 值最小的 ( 方格 ) 节点，然后对所选择的方格作如下操作：

- 把它从 open list 里取出，放到 close list 中。
- 检查所有与它相邻的方格，忽略其中在 close list 中或是不可走 (unwalkable) 的方格 ( 比如墙，水，或是其他非法地形 ) ，如果方格不在open lsit 中，则把它们加入到 open list 中。把我们选定的方格设置为这些新加入的方格的父亲。
  - 然后计算新加入的方格相对于当前处理方格的F、G、H值(注意G为累加值)
  - 选取其中F值最小的作为下一个待处理的方格。
  - 然后继续上面的操作。
- 如果某个相邻的**所有方格**均已经在 open list 中，则检查所有方格所在的这条路径是否更优，也就是说经由当前方格 ( 我们选中的方格 ) 到达那个方格是否具有更小的 G 值。
  - 如果没有，不做任何操作。
  - 相反，如果 G 值更小，则把那个方格的父亲设为当前方格 ( 我们选中的方格 ) ，然后重新计算那个方格的 F 值和 G 值。



![image004.jpg](../picture/10b76d7dfb9f62c0621f83409d472e23.jpeg)

1. 对于上图，在我们最初的 9 个方格中，还有 8 个在 open list 中，起点被放入了 close list 中。在这些方格中，起点右边的格子的 **F 值 40 最小**，因此我们选择这个方格作为下一个要处理的方格。它的外框用蓝线打亮。

2. 首先，我们把它从 open list 移到 close list 中  。然后我们检查与它相邻的方格。它右边的方格是墙壁，我们忽略。它左边的方格是起点，在 close list 中，我们也忽略。其他 4 个**相邻的方格均在 open list 中**，因此我们需要检查经由这个方格到达那里的路径是否更好，使用 G 值来判定。让我们看看上面的方格。它现在的 G 值为 14 。如果我们经由当前方格到达那里， G 值将会为 20。显然 20 比 14 大，因此这不是最优的路径。

3. 当把 4 个已经在 open list 中的相邻方格都检查后，**没有发现经由当前方格的更好路径**，因此我们不做任何改变。现在我们已经检查了当前方格的所有相邻的方格，并也对他们作了处理，是时候选择下一个待处理的方格了。

4. 因此再次遍历我们的 open list ，现在它只有 7 个方格了，我们需要选择 F 值最小的那个。有趣的是，这次有两个方格的 F 值都 54 ，选哪个呢？没什么关系。**从速度上考虑，选择最后加入 open list 的方格更快**。

5. 我们选择起点右下方的方格，如下图所示

   ![image005.jpg](../picture/19efd36ffafcb18f4a5af00078b07849.jpeg)

6. 只有三个方格可以选取，当前处理方格左边的方格，以及新加入的两个方格中。我们检查经由当前方格到达那里是否具有更小的 G 值。没有。因此我们准备从 open list 中选择下一个待处理的方格。

7. 以此类推，找到最短路径

![image007.jpg](../picture/a2a101733a0277b4346f75b0c3efd89f.jpeg)

相关代码如下

```python
def heuristic(a: GridLocation, b: GridLocation) -> float:
    (x1, y1) = a
    (x2, y2) = b
    return abs(x1 - x2) + abs(y1 - y2)

def a_star_search(graph: WeightedGraph, start: Location, goal: Location):
    frontier = PriorityQueue()
    frontier.put(start, 0)
    came_from: dict[Location, Optional[Location]] = {}
    cost_so_far: dict[Location, float] = {}
    came_from[start] = None
    cost_so_far[start] = 0
    
    while not frontier.empty():
        current: Location = frontier.get()
        
        if current == goal:
            break
        
        for next in graph.neighbors(current):
            new_cost = cost_so_far[current] + graph.cost(current, next)
            if next not in cost_so_far or new_cost < cost_so_far[next]:
                cost_so_far[next] = new_cost
                priority = new_cost + heuristic(next, goal)
                frontier.put(next, priority)
                came_from[next] = current
    
    return came_from, cost_so_far

from implementation import *
start, goal = (1, 4), (8, 3)
came_from, cost_so_far = a_star_search(diagram4, start, goal)
draw_grid(diagram4, point_to=came_from, start=start, goal=goal)
print()
draw_grid(diagram4, path=reconstruct_path(came_from, start=start, goal=goal))
```



## 历年小测

[第一次小测](\class_source\renji\1.pdf)
[第二次小测](\class_source\renji\2.pdf)
[第三次小测](\class_source\renji\3.pdf)
[第四次小测](\class_source\renji\4.pdf)
[第五次小测](\class_source\renji\5.pdf)
[第六次小测](\class_source\renji\6.pdf)